{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from random import choice\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from bert import modeling, optimization\n",
    "from transformers import BertConfig, BertForPreTraining, Trainer, TrainingArguments, InputFeatures, BatchEncoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "%env WANDB_WATCH=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = Path('/home/maxkvant/data/pretraining_dataset/prepr/cleaned/')\n",
    "repos = list(data_root_path.iterdir())\n",
    "train_repos, val_repos = train_test_split(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/maxkvant/data/pretraining_dataset/20200621_Python_github_python_minus_ethpy150open_deduplicated_vocabulary.txt') as vocab_file:\n",
    "    tokens = [token[1:-2] for token in vocab_file.readlines()]\n",
    "token_ids = {token: token_id for token_id, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_ID_MASKED = -100\n",
    "MASKING_PROBABILITY = .15\n",
    "\n",
    "\n",
    "def make_masked(input_ids):\n",
    "    return\n",
    "\n",
    "\n",
    "class TokenizedReposDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, repo_paths, token_ids):\n",
    "        self.file_paths = [\n",
    "            file_path \n",
    "            for repo_path in repo_paths\n",
    "            for file_path in repo_path.glob('**/*.json')\n",
    "        ]\n",
    "        self.token_ids = token_ids\n",
    "        def file_len(file_path):\n",
    "            try:\n",
    "                return len(json.load(file_path.open()))\n",
    "            except:\n",
    "                return 0\n",
    "        self.len = sum(map(file_len, self.file_paths))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for file_path in self.file_paths:\n",
    "            with file_path.open() as file:\n",
    "                try:\n",
    "                    file_lines = json.load(file)\n",
    "                except:\n",
    "                    continue\n",
    "                file_lines = [\n",
    "                    [self.token_ids[token] for token in line]\n",
    "                    for line in file_lines\n",
    "                ]\n",
    "                last_i = len(file_lines) - 1\n",
    "                \n",
    "                for i, line in enumerate(file_lines):\n",
    "                    next_label = np.random.rand() > .5 and i != last_i\n",
    "                    try:\n",
    "                        b_line = file_lines[i + 1] if next_label else choice(file_lines[:i] + file_lines[i + 1:])\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    type_ids = [0 for _ in line] + [1 for _ in b_line]\n",
    "                    input_ids = line + b_line\n",
    "                    masked_input = np.array(input_ids)\n",
    "                    mask = np.random.binomial(1, MASKING_PROBABILITY, len(input_ids)).astype(np.bool)\n",
    "                    masked_input[mask] = TOKEN_ID_MASKED\n",
    "                    if len(input_ids) <= 512:\n",
    "                        yield InputFeatures(input_ids, label=masked_input), next_label, type_ids\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train.txt', 'wt') as val_ds_file:\n",
    "#     for line_token_ids in TokenizedReposDataset(train_repos, token_ids):\n",
    "#         val_ds_file.write(' '.join(map(str, line_token_ids)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read(path):\n",
    "#     data = []\n",
    "#     with open(path) as f:\n",
    "#         for l in f.readlines():\n",
    "#             input_ids = list(map(int, l.split()))\n",
    "#             masked_input = np.array(input_ids)\n",
    "#             mask = np.random.binomial(1, MASKING_PROBABILITY, len(input_ids)).astype(np.bool)\n",
    "#             masked_input[mask] = TOKEN_ID_MASKED\n",
    "#             data.append(InputFeatures(input_ids, label=masked_input))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_ID_PAD = token_ids['<pad>_']\n",
    "\n",
    "\n",
    "def collate(data):\n",
    "    batch_width = max(len(dp.input_ids) for dp, _, _ in data)\n",
    "    inputs_ids, attention_masks, lang_masks, next_sentense_labels, token_type_ids = [], [], [], [], []\n",
    "    for dp, next_label, type_ids in data:\n",
    "        input_ids = dp.input_ids\n",
    "        line_len = len(input_ids)\n",
    "        pad_len = batch_width - line_len\n",
    "        inputs_ids.append(input_ids + [TOKEN_ID_PAD] * pad_len)\n",
    "        attention_masks.append([1] * line_len + [0] * pad_len)\n",
    "        lang_masks.append(np.concatenate((dp.label, np.zeros(pad_len, dtype=np.long))))\n",
    "        next_sentense_labels.append(next_label)\n",
    "        token_type_ids.append(type_ids + [0] * pad_len)\n",
    "    return BatchEncoding(data={\n",
    "        'input_ids': torch.LongTensor(inputs_ids), \n",
    "        'attention_mask': torch.LongTensor(attention_masks), \n",
    "        'labels': torch.LongTensor(lang_masks),\n",
    "        'next_sentence_label': torch.LongTensor(next_sentense_labels),\n",
    "        'token_type_ids': torch.LongTensor(token_type_ids)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForPreTraining(BertConfig(vocab_size=50000, return_dict=True))\n",
    "# model = BertForPreTraining.from_pretrained('pytorch_bert_training_09_14/checkpoint-12500/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir='10_20', \n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    dataloader_num_workers=6,\n",
    "    logging_steps=100,\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    run_name='10_20'\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=TokenizedReposDataset(train_repos, token_ids),\n",
    "    eval_dataset=TokenizedReposDataset(val_repos, token_ids),\n",
    "    data_collator=collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
