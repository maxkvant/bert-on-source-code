{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_json_dataset(path):\n",
    "    with path.open('r') as istream:\n",
    "        return [json.loads(l) for l in istream]\n",
    "\n",
    "\n",
    "def concat_json_datasets(data_folder):\n",
    "    data_folder = Path(data_folder)\n",
    "    return [\n",
    "        l for file in data_folder.glob(\"*.jsonl\")\n",
    "        for l in read_json_dataset(file)\n",
    "    ]\n",
    "\n",
    "\n",
    "DATA_FOLDER = Path.home() / \"data/method_name_prediction/python/final/jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  412178\n",
      "Val:  23107\n",
      "Test:  22176\n"
     ]
    }
   ],
   "source": [
    "train = concat_json_datasets(DATA_FOLDER / \"train\")\n",
    "valid = concat_json_datasets(DATA_FOLDER / \"valid\")\n",
    "test = concat_json_datasets(DATA_FOLDER / \"test\")\n",
    "\n",
    "print(\"Train: \", len(train))\n",
    "print(\"Val: \", len(valid))\n",
    "print(\"Test: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def remove_docstring(code: str, docstring: str):\n",
    "    doc_start_exp = re.compile(r\"^\\s*[ur]?[ur]?('''|\\\"\\\"\\\")\")\n",
    "    docstring = doc_start_exp.sub(\"\", docstring)\n",
    "    assert docstring in code\n",
    "    code = code.replace(docstring, \"\")\n",
    "    quotes_exp = re.compile(r\"[ur]?[ur]?('''\\s*'''|\\\"\\\"\\\"\\s*\\\"\\\"\\\")\")\n",
    "    return quotes_exp.sub(\"\", code)\n",
    "\n",
    "        \n",
    "def add_field(dicts: 'List[dict]', key, func):\n",
    "    for d in tqdm(dicts):\n",
    "        d[key] = func(d)\n",
    "\n",
    "\n",
    "def get_nth_token_drop_async(entry, n):\n",
    "    if entry['code_tokens'][0] == 'async':\n",
    "        return entry['code_tokens'][n + 1]\n",
    "    return entry['code_tokens'][n]\n",
    "\n",
    "\n",
    "def get_function_name_from_code_tokens(entry):\n",
    "    return get_nth_token_drop_async(entry, 1)\n",
    "\n",
    "\n",
    "def replace_with_placeholder(code, name, placeholder='_'):\n",
    "    name_exp = re.compile(name)  # should we ignorecase or smth like that?\n",
    "    return name_exp.sub(placeholder, code)\n",
    "\n",
    "\n",
    "def preprocess_function_body(entry, drop_docstring = True):\n",
    "    code = entry['code']\n",
    "    if drop_docstring:\n",
    "        code = remove_docstring(code, entry['docstring'])\n",
    "    try:\n",
    "        code = replace_with_placeholder(code, entry['function_name'])\n",
    "    except Exception:\n",
    "        return \"FAILED_REPLACING_PLACEHOLDER\" + code\n",
    "    return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412178/412178 [00:00<00:00, 754737.34it/s]\n",
      "100%|██████████| 23107/23107 [00:00<00:00, 683839.12it/s]\n",
      "100%|██████████| 22176/22176 [00:00<00:00, 786491.85it/s]\n"
     ]
    }
   ],
   "source": [
    "func_name_key = 'function_name'\n",
    "add_field(train, func_name_key, get_function_name_from_code_tokens)\n",
    "\n",
    "add_field(valid, func_name_key, get_function_name_from_code_tokens)\n",
    "\n",
    "add_field(test, func_name_key, get_function_name_from_code_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412178/412178 [00:44<00:00, 9189.00it/s] \n",
      "100%|██████████| 23107/23107 [00:02<00:00, 8314.68it/s]\n",
      "100%|██████████| 22176/22176 [00:02<00:00, 8920.90it/s]\n"
     ]
    }
   ],
   "source": [
    "body_key = 'function_body'\n",
    "add_field(train, body_key, preprocess_function_body)\n",
    "\n",
    "add_field(valid, body_key, preprocess_function_body)\n",
    "\n",
    "add_field(test, body_key, preprocess_function_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  412178\n",
      "Val:  23107\n",
      "Test:  22176\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: \", len(train))\n",
    "print(\"Val: \", len(valid))\n",
    "print(\"Test: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  412173\n",
      "Val:  23107\n",
      "Test:  22176\n"
     ]
    }
   ],
   "source": [
    "def filter_failed(entry, body_key=body_key):\n",
    "    body = entry[body_key]\n",
    "    return not (body.startswith(\"FAILED_REMOVING_COMMENTS\") \n",
    "                or body.startswith(\"FAILED_REPLACING_PLACEHOLDER\"))\n",
    "\n",
    "\n",
    "train = list(filter(filter_failed, train))\n",
    "valid = list(filter(filter_failed, valid))\n",
    "test = list(filter(filter_failed, test))\n",
    "\n",
    "print(\"Train: \", len(train))\n",
    "print(\"Val: \", len(valid))\n",
    "print(\"Test: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  412160\n",
      "Val:  23107\n",
      "Test:  22176\n"
     ]
    }
   ],
   "source": [
    "def filter_self_ref(entry, body_key=body_key, name_key=func_name_key):\n",
    "    return not (entry[name_key] in entry[body_key])\n",
    "\n",
    "\n",
    "train = list(filter(filter_self_ref, train))\n",
    "valid = list(filter(filter_self_ref, valid))\n",
    "test = list(filter(filter_self_ref, test))\n",
    "\n",
    "print(\"Train: \", len(train))\n",
    "print(\"Val: \", len(valid))\n",
    "print(\"Test: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cubert.python_tokenizer import PythonTokenizer\n",
    "from cubert.code_to_subtokenized_sentences import code_to_cubert_sentences\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "\n",
    "\n",
    "MODELS_DIR = Path.home() / \"models/cubert\"\n",
    "VOCAB_PATH = MODELS_DIR / \"github_python_minus_ethpy150open_deduplicated_vocabulary.txt\"\n",
    "\n",
    "\n",
    "python_tokenizer = PythonTokenizer()\n",
    "subword_tokenizer = text_encoder.SubwordTextEncoder(VOCAB_PATH.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def tokenize_cubert(key, entry):\n",
    "    return code_to_cubert_sentences(entry[key], python_tokenizer, subword_tokenizer)\n",
    "\n",
    "\n",
    "tokenize_body = partial(tokenize_cubert, body_key)\n",
    "tokenize_name = partial(tokenize_cubert, func_name_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = [e for e in train if 'Ne = len(magnetic_states)' in e['code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 38829/412160 [03:36<27:21, 227.47it/s]  WARNING:absl:The tokenizer raised exception `unindent does not match any outer indentation level (<tokenize>, line 15)` while parsing def _(resource=None, methods=[\"get\", \"post\", \"put\", \"delete\"],\n",
      "          schema=None):\n",
      "    \n",
      "    def _(func):\n",
      "        def wrapper(self, *args, **kwargs):\n",
      "            # \"test\" argument means no wrap func this time,\n",
      "            # return original func immediately.\n",
      "            if kwargs.get(\"test\", False):\n",
      "                kwargs.pop(\"test\")\n",
      "                func(self, *args, **kwargs)\n",
      "\n",
      "            _methods = methods\n",
      "            if isinstance(methods, str):\n",
      "                _methods = [methods]\n",
      "             = self.r.(resource)\n",
      "            for method in _methods:\n",
      "                getattr(, method)(func, schema)\n",
      "        # Ordered by declare sequence\n",
      "        # http://stackoverflow.com/questions/4459531/how-to-read-class-attributes-in-the-same-order-as-declared\n",
      "        f_locals = sys._getframe(1).f_locals\n",
      "        _order = len([v for v in f_locals.itervalues()\n",
      "                     if hasattr(v, '__call__') and\n",
      "                     hasattr(v, '__name__') and\n",
      "                     v.__name__ == \"wrapper\"])\n",
      "        wrapper.__dict__[\"_order\"] = _order\n",
      "        return wrapper\n",
      "    return _\n",
      " 10%|▉         | 39247/412160 [03:40<47:27, 130.94it/s]  WARNING:absl:The tokenizer raised exception `unindent does not match any outer indentation level (<tokenize>, line 4)` while parsing def (self, resource):\n",
      "        \n",
      "         = self.s.get(resource, Route(resource))\n",
      "        self.s.update({resource: })\n",
      "        return \n",
      " 27%|██▋       | 111841/412160 [09:29<24:41, 202.72it/s] WARNING:absl:The tokenizer raised exception `unindent does not match any outer indentation level (<tokenize>, line 11)` while parsing def _(self, hypernode, composing_nodes=set(), attr_dict=None, **attr):\n",
      "        \n",
      "        attr_dict = self._combine_attribute_arguments(attr_dict, attr)\n",
      "\n",
      "        # If the hypernode hasn't previously been added, add it along\n",
      "        # with its attributes\n",
      "        if not self.has_hypernode(hypernode):\n",
      "        \tattr_dict[\"__composing_nodes\"] = composing_nodes\n",
      "        \tadded_nodes = composing_nodes\n",
      "        \tremoved_nodes = set()\n",
      "            self._hypernode_attributes[hypernode] = attr_dict\n",
      "        # Otherwise, just update the hypernode's attributes\n",
      "        else:\n",
      "            self._hypernode_attributes[hypernode].update(attr_dict)\n",
      "            added_nodes = composing_nodes - self._hypernode_attributes\\\n",
      "            \t\t\t  [hypernode][\"__composing_nodes\"]\n",
      "\t\t\tremoved_nodes = self._hypernode_attributes\\\n",
      "            \t\t\t  \t[hypernode][\"__composing_nodes\"] - composing_nodes\n",
      "        \n",
      "        # For every \"composing node\" added to this hypernode, update\n",
      "        # those nodes attributes to be members of this hypernode\n",
      "\t\tfor node in added_nodes:\n",
      "\t\t\t___membership(node, hypernode)\n",
      "\t\t# For every \"composing node\" added to this hypernode, update\n",
      "        # those nodes attributes to no longer be members of this hypernode\n",
      "        for node in remove_nodes:\n",
      "\t\t\t_remove_hypernode_membership(node, hypernode)\n",
      " 44%|████▍     | 182431/412160 [15:36<14:31, 263.50it/s] WARNING:absl:The tokenizer raised exception `('EOF in multi-line statement', (5, 0))` while parsing def _(sed_inputs=sed_dict):\n",
      "    \n",
      "    return L_diffuser_outer(sed_inputs['tank']['W']) -\n",
      "            (2 * (sed_inputs['manifold']['diffuser']['thickness_wall']).to(u.m)).magnitude)\n",
      " 46%|████▌     | 189751/412160 [16:08<10:29, 353.49it/s] WARNING:absl:The tokenizer raised exception `unindent does not match any outer indentation level (<tokenize>, line 4)` while parsing def _(self, level=logging.DEBUG):\n",
      "        \n",
      "         = self.()\n",
      "        if  is None:\n",
      "            logger.debug(\"No items in: {}\".format(self))\n",
      "        else:\n",
      "            .logBranch(level=level)\n",
      " 83%|████████▎ | 341562/412160 [28:49<06:50, 171.82it/s]  WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (382, 4))` while parsing def _(self,meterPos,pos_i=None,slot_i=None,num_slots=None,all_positions=None,parse=None):\n",
      "\t\timport prosodic as p\n",
      "\t\t#if meterPos.slots[0].i<2:\n",
      "\t\t#\tprint meterPos.slots[0].word\n",
      "\n",
      "\t\t#print meterPos,pos_i,slot_i,num_slots,all_positions\n",
      "\t\t#prevpos=all_positions[pos_i-1]\n",
      "\t\t#print pos_i, meterPos, prevpos, pos_i,pos_i-1,all_positions, len(meterPos.slots)\n",
      "\n",
      "\t\tif '.' in self.name:\t# kiparsky self.names\n",
      "\t\t\t## load variables\n",
      "\n",
      "\t\t\t#exception for first foot\n",
      "\t\t\t#if 'skip_initial_foot' in parse.constraintNames:\n",
      "\t\t\t#\tif meterPos.slots[0].i<2:\n",
      "\t\t\t#\t\treturn 0\n",
      "\n",
      "\t\t\tif 'extrametrical-first-pos' in parse.constraintNames and pos_i==0:\n",
      "\t\t\t\treturn 0\n",
      "\t\t\telif 'skip_initial_foot' in parse.constraintNames and pos_i in [0,1]:\n",
      "\t\t\t\treturn 0\n",
      "\n",
      "\n",
      "\t\t\tpromSite = self.name.split(\".\")[1]\n",
      "\t\t\tpromType = self.name.split(\".\")[0]\n",
      "\t\t\tpromSite_meter = promSite.split(\"=>\")[0].strip()\t# s/w\n",
      "\t\t\tpromSite_prom = promSite.split(\"=>\")[1].strip()\t\t# +- u/p\n",
      "\n",
      "\t\t\tif meterPos.meterVal != promSite_meter:\t# then this constraint does not apply\n",
      "\t\t\t\treturn 0\n",
      "\n",
      "\t\t\tif promSite_prom[0:1] == \"-\":\t\t\t\t\t\t# -u or -p: eg, if s=>-u, then NOT EVEN ONE s can be u(nprom)\n",
      "\t\t\t\tpromSite_isneg = True\n",
      "\t\t\t\tpromSite_prom = promSite_prom[1:]\t\t\t\t# u or p\n",
      "\t\t\telse:\n",
      "\t\t\t\tpromSite_isneg = False\t\t\t\t\t\t\t# u or p: eg, if s=>p, then AT LEAST ONE s must be p(rom)\n",
      "\n",
      "\t\t\t\"\"\"\n",
      "\t\t\tRemoved 4/12/2017: apparently there was an option to restrict just 'P'rimary stresses\n",
      "\t\t\tBut required using an uppercase P in the meter config. This was nowhere stated elsewhere\n",
      "\t\t\tand has never been used. I'm disabling it. Let's just use a separate prominence type\n",
      "\t\t\tif we want to restrict only primary stresses.\n",
      "\n",
      "\t\t\tif promSite_prom.lower()==promSite_prom:\n",
      "\t\t\t\tpromSite_prom = (promSite_prom == 'p')\t\t\t\t# string 2 boolean: p:True, u:False\n",
      "\t\t\telse:\n",
      "\t\t\t\tif promSite_prom==\"P\":\n",
      "\t\t\t\t\tpromSite_prom=1.0\n",
      "\t\t\t\t#elif promSite_prom==\"U\":\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\tpromSite_prom=0.0\n",
      "\t\t\t\"\"\"\n",
      "\n",
      "\t\t\tpromSite_prom = (promSite_prom == 'p')\t\t\t\t# string 2 boolean: p:True, u:False\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t# NOT EVEN ONE unit_prom can be promSite_prom:\n",
      "\t\t\tif promSite_isneg:\n",
      "\t\t\t\tnumtrue=0\n",
      "\t\t\t\tfor slot in meterPos.slots:\n",
      "\t\t\t\t\tslot_prom=slot.feature('prom.'+promType,True)\n",
      "\t\t\t\t\tif slot_prom==None: continue\n",
      "\n",
      "\t\t\t\t\t#if type(promSite_prom)==type(True):\n",
      "\t\t\t\t\t#\tslot_prom=bool(slot_prom)\n",
      "\t\t\t\t\tpstress_thresh=self.meter.config.get('phrasal_stress_threshold',PSTRESS_THRESH_DEFAULT)\n",
      "\t\t\t\t\ttry:\n",
      "\t\t\t\t\t\tpstress_thresh=float(pstress_thresh)\n",
      "\t\t\t\t\texcept ValueError:\n",
      "\t\t\t\t\t\tpstress_thresh=PSTRESS_THRESH_DEFAULT\n",
      "\n",
      "\t\t\t\t\tbool_prom_type = bool(slot_prom) if promType!='phrasal_stress' else slot_prom<=pstress_thresh\n",
      "\n",
      "\t\t\t\t\tif bool_prom_type == promSite_prom:\n",
      "\t\t\t\t\t\t#numtrue+=float(slot_prom)\n",
      "\t\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t#return 2 if numtrue else 0\n",
      "\t\t\t\t#print self.weight, numtrue\n",
      "\t\t\t\t## CHANGED 10/10/2016: This constraint returns its weight\n",
      "\t\t\t\t## *times* the number of slots/syllables that violated it.\n",
      "\t\t\t\t## CHANGED 4/12/2017: numtrue is actually float of the prominence\n",
      "\t\t\t\t## so for phrasal stress is its p-stress value, for seconday stress is 0.5, etc.\n",
      "\n",
      "\t\t\t\treturn self.weight * numtrue\n",
      "\t\t\t\t#return 0\n",
      "\n",
      "\t\t\t# AT LEAST ONE unit_prom must be promSite_prom (or else, violate):\n",
      "\t\t\telse:\n",
      "\t\t\t\tviolated=True\n",
      "\t\t\t\tran=False\n",
      "\t\t\t\tfor slot in meterPos.slots:\n",
      "\t\t\t\t\tslot_prom=slot.feature('prom.'+promType,True)\n",
      "\t\t\t\t\tif slot_prom==None:\n",
      "\t\t\t\t\t\tcontinue\n",
      "\t\t\t\t\tran=True\n",
      "\t\t\t\t\tif bool(slot_prom)==promSite_prom:\n",
      "\t\t\t\t\t\tviolated=False\n",
      "\t\t\t\tif ran and violated:\n",
      "\t\t\t\t\treturn self.weight\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\treturn 0\n",
      "\n",
      "\t\telif self.name.lower().startswith('initialstrong'):\n",
      "\t\t\t#if meterPos.slots[0].i==0:\n",
      "\t\t\tif pos_i==0:\n",
      "\t\t\t\tif meterPos.meterVal == 's':\n",
      "\t\t\t\t\treturn self.weight\n",
      "\t\t\treturn 0\n",
      "\n",
      "\t\telif self.name.lower().startswith('functiontow'):\n",
      "\t\t\t#exception for first foot\n",
      "\t\t\tif p.config.get('skip_initial_foot',0):\n",
      "\t\t\t\tif meterPos.slots[0].i<2:\n",
      "\t\t\t\t\treturn 0\n",
      "\n",
      "\t\t\tif meterPos.meterVal != 's':\t# then this constraint does not apply\n",
      "\t\t\t\treturn 0\n",
      "\n",
      "\t\t\tvio = 0\n",
      "\t\t\tfor slot in meterPos.slots:\n",
      "\t\t\t\tif slot.word.feature('functionword'):\n",
      "\t\t\t\t\tvio += self.weight\n",
      "\t\t\treturn vio\n",
      "\n",
      "\t\telif self.name.lower().startswith('footmin'):\n",
      "\t\t\tif len(meterPos.slots) < 2:\n",
      "\t\t\t\treturn 0\n",
      "\t\t\telif len(meterPos.slots) > 2:\n",
      "\t\t\t\treturn self.weight\n",
      "\t\t\tname=self.name.lower()\n",
      "\t\t\ta = meterPos.slots[0]\n",
      "\t\t\tb = meterPos.slots[1]\n",
      "\n",
      "\t\t\t## should this apply to ALL foomin constraints?\n",
      "\t\t\t#if ( bool(a.feature('prom.stress',True)) and bool(b.feature('prom.stress',True))):\n",
      "\t\t\t#\treturn self.weight\n",
      "\t\t\t##\n",
      "\n",
      "\t\t\tif name=='footmin-nohx':\n",
      "\t\t\t\tif (bool(a.feature('prom.weight',True))):\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif name=='footmin-w-resolution':\n",
      "\t\t\t\tif a.word != b.word: return 0 # only applies within word-boundaries\n",
      "\t\t\t\tfirstsyll_islight=bool(a.feature('prom.weight',True)) == False\n",
      "\t\t\t\tfirstsyll_isstressed=bool(a.feature('prom.stress',True)) == True\n",
      "\t\t\t\tif not (firstsyll_islight and firstsyll_isstressed):\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif name=='footmin-f-resolution':\n",
      "\t\t\t\tif a.word == b.word: return 0 # only applies to word-boundaries\n",
      "\t\t\t\tif meterPos.meterVal=='s': return self.weight # cannot apply to strong positions\n",
      "\t\t\t\ta_is_fw = bool(a.word.feature('functionword'))\n",
      "\t\t\t\tb_is_fw = bool(b.word.feature('functionword'))\n",
      "\t\t\t\tif not (a_is_fw and b_is_fw): return self.weight\n",
      "\n",
      "\t\t\tif name=='footmin-s-nohx':\n",
      "\t\t\t\tif meterPos.meterVal=='s':\n",
      "\t\t\t\t\tif bool(a.feature('prom.weight',True)) or a.word!=b.word:\n",
      "\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif \"nolh\" in name:\n",
      "\t\t\t\tif ( (bool(b.feature('prom.weight',True))) ):\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif \"strongconstraint\" in name:\n",
      "\t\t\t\tif bool(b.feature('prom.strength',True)):\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\t\tif bool(a.feature('prom.strength',True)):\n",
      "\t\t\t\t\tif not bool(a.feature('prom.weight',True)):\n",
      "\t\t\t\t\t\t if a.word==b.word and not a.wordpos[0]==a.wordpos[1]:\n",
      "\t\t\t\t\t\t \tif not bool(b.feature('prom.stress',True)):\n",
      "\t\t\t\t\t\t \t\treturn 0\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif name=='footmin-none':\n",
      "\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif name=='footmin-none-unless-in-first-two-positions':\n",
      "\t\t\t\tif pos_i!=0 and pos_i!=1:\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif name=='footmin-none-unless-in-second-position':\n",
      "\t\t\t\tif pos_i!=1:\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\tif name=='footmin-no-s': return self.weight * int(meterPos.meterVal=='s')\n",
      "\t\t\tif name=='footmin-no-w': return self.weight * int(meterPos.meterVal=='w')\n",
      "\n",
      "\t\t\tif name=='footmin-no-s-unless-preceded-by-ww':\n",
      "\t\t\t\t# @TODO: bug when number of syllables in maxW is > 2 ?\n",
      "\t\t\t\tif meterPos.meterVal!='s': return 0\n",
      "\t\t\t\tif pos_i==0: return self.weight\n",
      "\t\t\t\tprevpos=all_positions[pos_i-1]\n",
      "\t\t\t\t#print pos_i, meterPos, prevpos, pos_i,pos_i-1,all_positions\n",
      "\t\t\t\tif len(prevpos.slots)>1 and prevpos.meterVal=='w':\n",
      "\t\t\t\t\treturn 0\n",
      "\t\t\t\treturn self.weight\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tif \"wordbound\" in name:\n",
      "\t\t\t\tif name=='footmin-wordbound':\n",
      "\t\t\t\t\tif a.word!=b.word:\n",
      "\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\t\tif \"nomono\" in name:\n",
      "\t\t\t\t\tif (a.word.numSyll==1 or b.word.numSyll==1):\n",
      "\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\t\tif 'lexmono' in name:\n",
      "\t\t\t\t\t#if a.word.numSyll==1 and a.word.stress==\"P\"\n",
      "\t\t\t\t\tif a.word.isLexMono() or b.word.isLexMono():\n",
      "\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t## everyone is happy if both are function words\n",
      "\t\t\t\tif a.word.feature('functionword') and b.word.feature('functionword'):\n",
      "\t\t\t\t\treturn 0\n",
      "\n",
      "\t\t\t\tif a.word!=b.word:\n",
      "\t\t\t\t\tif \"bothnotfw\" in name:\n",
      "\t\t\t\t\t\tif not (a.word.feature('functionword') and b.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t\telif \"neitherfw\":\n",
      "\t\t\t\t\t\tif not (a.word.feature('functionword') or b.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t\telif \"leftfw\":\n",
      "\t\t\t\t\t\tif not (a.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t\telif \"rightfw\":\n",
      "\t\t\t\t\t\tif not (b.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t# only remaining possibilities:\n",
      "\t\t\t\t#\ti) slots a,b are from the same word\n",
      "\t\t\t\t#   ii) slots a,b are from contiguous words which are the same (haPPY HAppy)\n",
      "\n",
      "\t\t\t\tif a.wordpos[0]==a.wordpos[1]:\t# in the firs slot's (start,end) wordpos : if (start==end) :  then poss. (ii) above\n",
      "\t\t\t\t\tif \"bothnotfw\" in name:\n",
      "\t\t\t\t\t\tif not (a.word.feature('functionword') and b.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t\telif \"neitherfw\":\n",
      "\t\t\t\t\t\tif not (a.word.feature('functionword') or b.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t\telif \"leftfw\":\n",
      "\t\t\t\t\t\tif not (a.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t\telif \"rightfw\":\n",
      "\t\t\t\t\t\tif not (b.word.feature('functionword')):\n",
      "\t\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\t\t# poss. (i) remains\n",
      "\t\t\t\treturn 0\n",
      "\n",
      "\n",
      "\t\t## Constraints about words\n",
      "\t\tif self.name=='word-elision':\n",
      "\t\t\twords=set([slot.word for slot in meterPos.slots if hasattr(slot.word,'is_elision') and slot.word.is_elision])\n",
      "\t\t\tsylls=[]\n",
      "\t\t\tfor slot in meterPos.slots: sylls+=slot.children\n",
      "\n",
      "\t\t\tfor word in words:\n",
      "\t\t\t\tlastsyll=word.children[-1]\n",
      "\t\t\t\tif lastsyll in sylls: # only break if this position contains the word's final syllable\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\n",
      "\t\t# is this the end?\n",
      "\t\tis_end = slot_i+1==num_slots and meterPos.slots==all_positions[-1].slots\n",
      "\n",
      "\t\t## CONSTRAINTS ON PREVIOUS POSITIONS\n",
      "\t\t\n",
      "\n",
      "\t\tif self.name=='attridge-ss-not-by-ww':\n",
      "\t\t\t#if meterPos.meterVal!='s': return 0\n",
      "\t\t\t#if not is_end and meterPos.meterVal2 == 'ss':\n",
      "\t\t\t#\tparse.pauseComparisons=True\n",
      "\n",
      "\t\t\tif pos_i==0: return 0\n",
      "\t\t\tprevpos=all_positions[pos_i-1]\n",
      "\t\t\tprevprevpos=all_positions[pos_i-2] if (pos_i-2)>=0 else None\n",
      "\t\t\t#print prevprevpos,prevpos,meterPos\n",
      "\t\t\t#print prevprevpos.meterVal2 if prevprevpos else None,prevpos.meterVal2, meterPos.meterVal2\n",
      "\n",
      "\t\t\t#print prevprevpos,prevpos,meterPos\n",
      "\t\t\t#print prevprevpos.meterVal2 if prevprevpos else None,prevpos.meterVal2, meterPos.meterVal2\n",
      "\t\t\t#print dir(prevprevpos) if prevprevpos else None\n",
      "\t\t\t#print dir(prevpos) if prevprevpos else None\n",
      "\t\t\t#print dir(meterPos)\n",
      "\t\t\t#print\n",
      "\n",
      "\t\t\tif prevpos.meterVal2 == 'ss':\n",
      "\n",
      "\t\t\t\t#if (prevprevpos and prevprevpos.meterVal2=='ww')\n",
      "\n",
      "\t\t\t\tif (prevprevpos and prevprevpos.meterVal2=='ww') and (not hasattr(prevprevpos,'_flag_already_served_as_ww')):\n",
      "\t\t\t\t\tprevprevpos._flag_already_served_as_ww=True\n",
      "\t\t\t\t\tpass\n",
      "\t\t\t\telif meterPos.meterVal2=='ww' and (not hasattr(meterPos,'_flag_already_served_as_ww')):\n",
      "\t\t\t\t\tmeterPos._flag_already_served_as_ww=True\n",
      "\t\t\t\t\tpass\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\t#print 'ERROR!'\n",
      "\t\t\t\t\tfor cnstr in prevpos.constraintScores:\n",
      "\t\t\t\t\t\tif cnstr.name==self.name:\n",
      "\t\t\t\t\t\t\tprevpos.constraintScores[cnstr]=self.weight\n",
      "\t\t\t\t\t\t\tparse.constraintScores[cnstr]+=self.weight\n",
      "\n",
      "\t\t\t\t#parse.pauseComparisons=False\n",
      "\t\t\telif is_end and meterPos.meterVal2=='ss':\n",
      "\t\t\t\t#parse.pauseComparisons=False\n",
      "\t\t\t\tif prevpos.meterVal2=='ww':\n",
      "\t\t\t\t\tpass\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\t#print 'ERROR!'\n",
      "\t\t\t\t\treturn self.weight\n",
      "\t\t\t#print\n",
      "\t\t#\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "\t\t## POST HOC CONSTRAINTS\n",
      "\t\tif is_end:\n",
      "\t\t\tfinal_meter_str=''.join([''.join(pos.meterVal for slot in pos.slots) for pos in all_positions])\n",
      "\t\t\t#print final_meter_str\n",
      "\n",
      "\t\t\t# headedness\n",
      "\t\t\tif self.name.startswith('headedness'):\n",
      "\t\t\t\tshouldbe = self.name.split('!=')[-1]\n",
      "\n",
      "\t\t\t\t\"\"\"\n",
      "\t\t\t\tApproach 1: This approach doesn't really work on individual lines:\n",
      "\n",
      "\t\t\t\t# binary or ternary?\n",
      "\t\t\t\tweak_pos = [pos for pos in all_positions if pos.meterVal=='w']\n",
      "\t\t\t\tif len(weak_pos)<2: return 0\n",
      "\t\t\t\tweak_pos_types = [''.join('w' for slot in pos.slots) for pos in weak_pos]\n",
      "\n",
      "\t\t\t\tif weak_pos_types.count('ww')>weak_pos_types.count('w'): # ternary\n",
      "\t\t\t\t\tif final_meter_str[3]=='w': # anapestic\n",
      "\t\t\t\t\t\theadedness = 'rising'\n",
      "\t\t\t\t\telse: # dactylic\n",
      "\t\t\t\t\t\theadedness = 'falling'\n",
      "\t\t\t\telse: # binary\n",
      "\t\t\t\t\tif final_meter_str[3]=='w':\n",
      "\t\t\t\t\t\theadedness = 'falling' # trochaic\n",
      "\t\t\t\t\telse:\n",
      "\t\t\t\t\t\theadedness = 'rising'\n",
      "\n",
      "\t\t\t\tif shouldbe != headedness:\n",
      "\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t\n",
      "\t\t\t\tApproach 2: count 'ws' vs 'sw' pairs and give categorical violation\n",
      "\t\t\t\t\"\"\"\n",
      "\t\t\t\tquasi_feet=[''.join(x) for x in tools.slice([pos.meterVal for pos in all_positions],slice_length=2,runts=False)]\n",
      "\t\t\t\theadedness = 'rising' if quasi_feet.count('ws')>=quasi_feet.count('sw') else 'falling'\n",
      "\t\t\t\t#print final_meter_str\n",
      "\t\t\t\t#print quasi_feet\n",
      "\t\t\t\t#print headedness\n",
      "\t\t\t\t#print\n",
      "\n",
      "\t\t\t\tif shouldbe != headedness:\n",
      "\t\t\t\t\treturn self.weight\n",
      "\t\t\t\t#\n",
      "\t\t\t\tApproach 3: count 'ws' vs 'sw' pairs and give violation/num-pos per off foot\n",
      "\n",
      "\t\t\t\tquasi_feet=[''.join(x) for x in tools.slice([pos.meterVal for pos in all_positions],slice_length=2,runts=True)]\n",
      "\t\t\t\tif shouldbe == 'rising':\n",
      "\t\t\t\t\tnum_not_rising = float(len([ft for ft in quasi_feet if ft!='ws']))\n",
      "\t\t\t\t\treturn num_not_rising / float(len(all_positions)) * float(self.weight)\n",
      "\t\t\t\telif shouldbe == 'falling':\n",
      "\t\t\t\t\tnum_not_falling = float(len([ft for ft in quasi_feet if ft!='sw']))\n",
      "\t\t\t\t\treturn num_not_falling / float(len(all_positions)) * float(self.weight)\n",
      "\t\t\t\t\"\"\"\n",
      "\n",
      "\n",
      "\t\t\t# number of feet\n",
      "\t\t\tif self.name.startswith('number_feet'):\n",
      "\t\t\t\tshouldbe = int(self.name.split('!=')[-1])\n",
      "\t\t\t\tstrong_pos = [pos for pos in all_positions if pos.meterVal=='s']\n",
      "\t\t\t\tnum_feet = len(strong_pos) # debatable\n",
      "\t\t\t\tif shouldbe != num_feet:\n",
      "\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\t# other posthoc constraints\n",
      "\t\t\tif self.name.startswith('posthoc'):\n",
      "\t\t\t\tif self.name=='posthoc-no-final-ww':\n",
      "\t\t\t\t\tif len(all_positions[-1].slots)>1 and all_positions[-1].meterVal=='w':\n",
      "\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\t\tif self.name=='posthoc-no-final-w':\n",
      "\t\t\t\t\tif all_positions[-1].meterVal=='w':\n",
      "\t\t\t\t\t\treturn self.weight\n",
      "\n",
      "\t\t\t\tif self.name=='posthoc-standardize-weakpos':\n",
      "\t\t\t\t\tweak_pos = [pos for pos in all_positions if pos.meterVal=='w']\n",
      "\t\t\t\t\tif len(weak_pos)<2: return 0\n",
      "\t\t\t\t\tweak_pos_types = [''.join('w' for slot in pos.slots) for pos in weak_pos]\n",
      "\t\t\t\t\tmaxcount = max([weak_pos_types.count(wtype) for wtype in set(weak_pos_types)])\n",
      "\t\t\t\t\tdiff = len(weak_pos) - maxcount\n",
      "\t\t\t\t\treturn self.weight*diff\n",
      "\n",
      "\t\t# made it through this minefield, eh?\n",
      "\t\treturn 0\n",
      " 86%|████████▌ | 354229/412160 [29:52<03:13, 298.97it/s] WARNING:absl:The tokenizer raised exception `unindent does not match any outer indentation level (<tokenize>, line 3)` while parsing def _(self, s):\n",
      "         '\n",
      "        self.rv.append(Token(type='INTEGER', attr=s))\n",
      " 95%|█████████▍| 391102/412160 [32:39<01:51, 189.01it/s] WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (3, 4))` while parsing def _(a, sigma, rho, size=1):\n",
      "    \n",
      "    \"\"\"\n",
      "    f = utils.ar1\n",
      "    if np.isscalar(a):\n",
      "        r = f(rho, 0, sigma, size)\n",
      "    else:\n",
      "        n = len(a)\n",
      "        r = [f(rho, 0, sigma, n) for i in range(size)]\n",
      "        if size == 1:\n",
      "            r = r[0]\n",
      "    return a * np.exp(r)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (3, 4))` while parsing def _(x, a, sigma, rho):\n",
      "    \n",
      "    \"\"\"\n",
      "    return flib.arlognormal(x, np.log(a), sigma, rho, beta=1)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, alpha, beta, n):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    return flib._(x, alpha, beta, n)\n",
      " 95%|█████████▍| 391124/412160 [32:39<01:52, 187.68it/s]WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, p):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    p = np.atleast_2d(p)\n",
      "    if np.any(abs(np.sum(p, 1) - 1) > 0.0001):\n",
      "        print_(\"Probabilities in _ sum to\", np.sum(p, 1))\n",
      "    return flib.categorical(np.array(x).astype(int), p)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, k):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    x = np.atleast_1d(x)\n",
      "    return sum(np.log([i == k for i in x]))\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, theta):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    x = np.atleast_2d(x)\n",
      "    theta = np.atleast_2d(theta)\n",
      "    if (np.shape(x)[-1] + 1) != np.shape(theta)[-1]:\n",
      "        raise ValueError('The dimension of x in _ must be k-1.')\n",
      "    return flib.dirichlet(x, theta)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, alpha, k, loc=0, scale=1):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    return flib.exponweib(x, alpha, k, loc, scale)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, xi, mu=0, sigma=1):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    return flib.gev(x, xi, mu, sigma)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (3, 4))` while parsing def _(x, alpha, beta):\n",
      "    \n",
      "    \"\"\"\n",
      "\n",
      "    x = np.atleast_1d(x)\n",
      "    if sum(x.ravel() < 0):\n",
      "        return -inf\n",
      "    return flib.cauchy(x, alpha, beta) + len(x) * np.log(2)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (3, 4))` while parsing def _(x, n, m, N):\n",
      "    \n",
      "    \"\"\"\n",
      "\n",
      "    return flib.hyperg(x, n, m, N)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (3, 4))` while parsing def _(x, mu, tau):\n",
      "    \n",
      "    \"\"\"\n",
      "\n",
      "    return flib.gamma(np.abs(np.array(x) - mu), 1, tau) - \\\n",
      "        np.size(x) * np.log(2)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, n, p):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    # flib expects 2d arguments. Do we still want to support multiple p\n",
      "    # values along realizations ?\n",
      "    x = np.atleast_2d(x)\n",
      "    p = np.atleast_2d(p)\n",
      "\n",
      "    return flib.multinomial(x, n, p)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, mu, tau):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    # TODO: Vectorize in Fortran\n",
      "    if len(np.shape(x)) > 1:\n",
      "        return np.sum([flib.prec_mvnorm(r, mu, tau) for r in x])\n",
      "    else:\n",
      "        return flib.prec_mvnorm(x, mu, tau)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, mu, C):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    # TODO: Vectorize in Fortran\n",
      "    if len(np.shape(x)) > 1:\n",
      "        return np.sum([flib.cov_mvnorm(r, mu, C) for r in x])\n",
      "    else:\n",
      "        return flib.cov_mvnorm(x, mu, C)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, mu, sig):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    # TODO: Vectorize in Fortran\n",
      "    if len(np.shape(x)) > 1:\n",
      "        return np.sum([flib.chol_mvnorm(r, mu, sig) for r in x])\n",
      "    else:\n",
      "        return flib.chol_mvnorm(x, mu, sig)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, mu, alpha):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    alpha = np.array(alpha)\n",
      "    if (alpha > 1e10).any():\n",
      "        if (alpha > 1e10).all():\n",
      "            # Return Poisson when alpha gets very large\n",
      "            return flib.poisson(x, mu)\n",
      "\n",
      "        # Split big and small dispersion values\n",
      "        big = alpha > 1e10\n",
      "        return flib.poisson(x[big], mu[big]) + flib.negbin2(x[big - True],\n",
      "                                                            mu[big - True], alpha[big - True])\n",
      "    return flib.negbin2(x, mu, alpha)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, alpha, m, b):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    return flib.truncated_pareto(x, alpha, m, b)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (3, 4))` while parsing def _(x, mu, tau, a=None, b=None):\n",
      "    \n",
      "    \"\"\"\n",
      "    x = np.atleast_1d(x)\n",
      "    if a is None:\n",
      "        a = -np.inf\n",
      "    a = np.atleast_1d(a)\n",
      "    if b is None:\n",
      "        b = np.inf\n",
      "    b = np.atleast_1d(b)\n",
      "    mu = np.atleast_1d(mu)\n",
      "    sigma = (1. / np.atleast_1d(np.sqrt(tau)))\n",
      "    if (x < a).any() or (x > b).any():\n",
      "        return -np.inf\n",
      "    else:\n",
      "        n = len(x)\n",
      "        phi = normal_like(x, mu, tau)\n",
      "        lPhia = utils.normcdf((a - mu) / sigma, log=True)\n",
      "        lPhib = utils.normcdf((b - mu) / sigma, log=True)\n",
      "        try:\n",
      "            d = utils.log_difference(lPhib, lPhia)\n",
      "        except ValueError:\n",
      "            return -np.inf\n",
      "        # d = np.log(Phib-Phia)\n",
      "        if len(d) == n:\n",
      "            Phi = d.sum()\n",
      "        else:\n",
      "            Phi = n * d\n",
      "        if np.isnan(Phi) or np.isinf(Phi):\n",
      "            return -np.inf\n",
      "        return phi - Phi\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (3, 4))` while parsing def _(x, mu, tau, alpha):\n",
      "    \n",
      "    \"\"\"\n",
      "    return flib.sn_like(x, mu, tau, alpha)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, nu):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    nu = np.asarray(nu)\n",
      "    return flib.t(x, nu)\n",
      "WARNING:absl:The tokenizer raised exception `('EOF in multi-line string', (4, 4))` while parsing def _(x, mu, lam, nu):\n",
      "    \n",
      "\n",
      "    \"\"\"\n",
      "    mu = np.asarray(mu)\n",
      "    lam = np.asarray(lam)\n",
      "    nu = np.asarray(nu)\n",
      "    return flib.nct(x, mu, lam, nu)\n",
      "100%|██████████| 412160/412160 [34:19<00:00, 200.11it/s]\n",
      "100%|██████████| 23107/23107 [01:55<00:00, 199.34it/s]\n",
      "100%|██████████| 22176/22176 [01:41<00:00, 218.39it/s]\n"
     ]
    }
   ],
   "source": [
    "add_field(train, body_key + \"_tokenized\", tokenize_body)\n",
    "add_field(valid, body_key + \"_tokenized\", tokenize_body)\n",
    "add_field(test, body_key + \"_tokenized\", tokenize_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412160/412160 [01:08<00:00, 5983.05it/s]\n",
      "100%|██████████| 23107/23107 [00:03<00:00, 6385.77it/s]\n",
      "100%|██████████| 22176/22176 [00:03<00:00, 6687.18it/s]\n"
     ]
    }
   ],
   "source": [
    "add_field(train, func_name_key + \"_tokenized\", tokenize_name)\n",
    "add_field(valid, func_name_key + \"_tokenized\", tokenize_name)\n",
    "add_field(test, func_name_key + \"_tokenized\", tokenize_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(path, json_list):\n",
    "    with open(path, 'x') as istream:\n",
    "        for j in json_list:\n",
    "            istream.write(f\"{json.dumps(j)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(DATA_FOLDER / \"train_preprocessed.jsonl\", train)\n",
    "save_jsonl(DATA_FOLDER / \"valid_preprocessed.jsonl\", valid)\n",
    "save_jsonl(DATA_FOLDER / \"test_preprocessed.jsonl\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    with open(path, 'r') as istream:\n",
    "        return [json.loads(line) for line in istream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p = read_jsonl(DATA_FOLDER / \"train_preprocessed.jsonl\")\n",
    "valid_p = read_jsonl(DATA_FOLDER / \"valid_preprocessed.jsonl\")\n",
    "test_p = read_jsonl(DATA_FOLDER / \"test_preprocessed.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def is_correct_python_code(code: str) -> bool:\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412160/412160 [00:43<00:00, 9369.34it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "failed_train = [\n",
    "    e for e in tqdm(train_p)\n",
    "    if not is_correct_python_code(e['function_body'])\n",
    "]\n",
    "len(failed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23107/23107 [00:02<00:00, 10005.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_valid = [\n",
    "    e for e in tqdm(valid_p)\n",
    "    if not is_correct_python_code(e['function_body'])\n",
    "]\n",
    "len(failed_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22176/22176 [00:02<00:00, 10831.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_test = [\n",
    "    e for e in tqdm(test_p)\n",
    "    if not is_correct_python_code(e['function_body'])\n",
    "]\n",
    "len(failed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_is_correct_python_code(entry):\n",
    "    return is_correct_python_code(entry['function_body'])\n",
    "\n",
    "\n",
    "train_p = list(filter(body_is_correct_python_code, train_p))\n",
    "valid_p = list(filter(body_is_correct_python_code, valid_p))\n",
    "test_p = list(filter(body_is_correct_python_code, test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  407429\n",
      "Val:  22762\n",
      "Test:  21877\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: \", len(train_p))\n",
    "print(\"Val: \", len(valid_p))\n",
    "print(\"Test: \", len(test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(path, json_list):\n",
    "    with open(path, 'w') as istream:\n",
    "        for j in json_list:\n",
    "            istream.write(f\"{json.dumps(j)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(DATA_FOLDER / \"train_preprocessed.jsonl\", train_p)\n",
    "save_jsonl(DATA_FOLDER / \"valid_preprocessed.jsonl\", valid_p)\n",
    "save_jsonl(DATA_FOLDER / \"test_preprocessed.jsonl\", test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
