{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.putenv(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "\n",
    "\n",
    "from transformers import BertConfig, EncoderDecoderModel, EncoderDecoderConfig\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, Iterable, List\n",
    "from pathlib import Path\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "import json\n",
    "import torch\n",
    "from catalyst import dl\n",
    "import re\n",
    "from catalyst.utils import set_global_seed\n",
    "import random\n",
    "\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, 'r') as istream:\n",
    "        return [json.loads(l) for l in istream]\n",
    "\n",
    "\n",
    "class TokenToIndexConverter:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_path: Union[str, Path],\n",
    "        unk_token = '[UNK]_',\n",
    "        bos_token = '\\\\u\\\\u\\\\uNL\\\\u\\\\u\\\\u_',\n",
    "        eos_token = '\\\\u\\\\u\\\\uNEWLINE\\\\u\\\\u\\\\u_',\n",
    "        pad_token = '<pad>_'\n",
    "    ):\n",
    "        vocab_path = Path(vocab_path)\n",
    "        self.subword_tokenizer = text_encoder.SubwordTextEncoder(vocab_path.as_posix())\n",
    "        self.token_to_index_map = {\n",
    "            tok: i\n",
    "            for i, tok in enumerate(self.subword_tokenizer.all_subtoken_strings)\n",
    "        }\n",
    "        self.index_to_token_map = {v: k for k, v in self.token_to_index_map.items()}\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = self[unk_token]\n",
    "        self.bos_token = bos_token\n",
    "        self.bos_index = self[bos_token]\n",
    "        self.eos_token = eos_token\n",
    "        self.eos_index = self[eos_token]\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = self[pad_token]\n",
    "\n",
    "        self.bos_exp = re.compile(r\"^(___NL___)*\")  # Remove any number of repeating newline characters\n",
    "        self.eos_exp = re.compile(r\"___NEWLINE___.*$\")  # Remove evrythin after first eod\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return (self.token_to_index_map[key] \n",
    "                if key in self.token_to_index_map \n",
    "                else self.unk_index)\n",
    "\n",
    "    def encode(self, tokens: str) -> List[int]:\n",
    "        return [self[tok] for tok in tokens]\n",
    "    \n",
    "    def encode_code(self, code: List[List[str]]) -> List[int]:\n",
    "        return [self.bos_index] + [\n",
    "            tok for line in code \n",
    "            for tok in self.encode(line)\n",
    "        ]\n",
    "    \n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        text = self.subword_tokenizer.decode(tokens, strip_extraneous=True)\n",
    "        text = self.bos_exp.sub(\"\", text)\n",
    "        text = self.eos_exp.sub(\"\", text)\n",
    "        return text\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.subword_tokenizer.vocab_size\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"seq2seq-transformer\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    #  bert config:\n",
    "    vocab_size: int\n",
    "    pad_token_id: int\n",
    "    hidden_size: int = 1024\n",
    "    num_attention_heads: int = 16\n",
    "    intermediate_size: int = 4096\n",
    "    max_position_embeddings: int = 512\n",
    "    \n",
    "    encoder_num_hidden_layers: int = 6\n",
    "    decoder_num_hidden_layers: int = 2\n",
    "\n",
    "    #  optimization:\n",
    "    max_lr: float = 5e-4\n",
    "    batch_size: int = 32\n",
    "    accumulation_steps: int = 16\n",
    "    \n",
    "    weight_decay: float = 0\n",
    "        \n",
    "    num_epochs: int = 50\n",
    "    patience: int = 5\n",
    "\n",
    "    #  lr scheduling:\n",
    "    warmup_prop: float = 0.15\n",
    "        \n",
    "    #  generation parameters:\n",
    "    eval_set_size: int = 2000\n",
    "    num_return_sequences: int = 5\n",
    "\n",
    "    logdir: str = f'logdir_{EXPERIMENT_NAME}'\n",
    "    resume: str = None\n",
    "        \n",
    "    seed: int = 19\n",
    "\n",
    "\n",
    "def make_model(config):\n",
    "    encoder_config = BertConfig(\n",
    "        vocab_size=config.vocab_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_hidden_layers=config.encoder_num_hidden_layers,\n",
    "        num_attention_heads=config.num_attention_heads,\n",
    "        intermediate_size=config.intermediate_size,\n",
    "        max_position_embeddings=config.max_position_embeddings,\n",
    "        pad_token_id=config.pad_token_id\n",
    "    )\n",
    "    decoder_config = BertConfig(\n",
    "        vocab_size=config.vocab_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_hidden_layers=config.decoder_num_hidden_layers,\n",
    "        num_attention_heads=config.num_attention_heads,\n",
    "        intermediate_size=config.intermediate_size,\n",
    "        max_position_embeddings=config.max_position_embeddings,\n",
    "        pad_token_id=config.pad_token_id\n",
    "    )\n",
    "    model = EncoderDecoderModel(\n",
    "        config=EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "MODELS_DIR = Path.home() / \"models/cubert\"\n",
    "\n",
    "\n",
    "token_to_index = TokenToIndexConverter(\n",
    "    (MODELS_DIR / \"github_python_minus_ethpy150open_deduplicated_vocabulary.txt\").as_posix()\n",
    ")\n",
    "config = Config(vocab_size=token_to_index.vocab_size, pad_token_id=token_to_index.pad_index)\n",
    "\n",
    "set_global_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path.home() / \"data/method_name_prediction/python/final/jsonl\"\n",
    "\n",
    "\n",
    "train = read_jsonl(DATA_FOLDER / \"train_preprocessed.jsonl\")\n",
    "valid = read_jsonl(DATA_FOLDER / \"valid_preprocessed.jsonl\")\n",
    "test = read_jsonl(DATA_FOLDER / \"test_preprocessed.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import Callable, Iterable, Optional\n",
    "\n",
    "\n",
    "class SequenceToSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_stream: Iterable['T'],\n",
    "        src_encoder: Callable[['T'], Tensor],\n",
    "        ref_stream: Iterable['T'],\n",
    "        ref_encoder: Callable[['T'], Tensor],\n",
    "        src_pad_index: int,\n",
    "        ref_pad_index: Optional[int] = None\n",
    "    ):\n",
    "        self.src = [src_encoder(s) for s in src_stream]\n",
    "        self.ref = [ref_encoder(s) for s in ref_stream]\n",
    "        assert len(self.src) == len(self.ref)\n",
    "        self.src_pad_index = src_pad_index\n",
    "        self.ref_pad_index = ref_pad_index if ref_pad_index is not None else src_pad_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.ref[idx]\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        src_batch, ref_batch = zip(*data)\n",
    "        input_ids = pad_sequence(\n",
    "            src_batch,\n",
    "            padding_value=self.src_pad_index,\n",
    "            batch_first=True\n",
    "        )\n",
    "        attention_mask = input_ids != self.src_pad_index\n",
    "        decoder_input_ids = pad_sequence(\n",
    "            ref_batch,\n",
    "            padding_value=self.ref_pad_index,\n",
    "            batch_first=True\n",
    "        )\n",
    "        labels = decoder_input_ids[:,1:]\n",
    "        decoder_input_ids = decoder_input_ids[:,:-1]\n",
    "        decoder_attention_mask = decoder_input_ids != self.ref_pad_index\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'decoder_attention_mask': decoder_attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def make_loader(self, *args, **kwargs):\n",
    "        return DataLoader(self, *args, collate_fn=self.collate_fn, **kwargs)\n",
    "\n",
    "\n",
    "def get_method_name_dataset(data, token_to_index, pad_index, max_length):\n",
    "\n",
    "    def truncated_encoder(encoder, max_length):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return encoder(*args, **kwargs)[:max_length]\n",
    "        return wrapper\n",
    "\n",
    "    def to_torch_encoder(encoder):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return LongTensor(encoder(*args, **kwargs))\n",
    "        return wrapper\n",
    "\n",
    "    return SequenceToSequenceDataset(\n",
    "        src_stream = (e['function_body_tokenized'] for e in data),\n",
    "        src_encoder = to_torch_encoder(\n",
    "            truncated_encoder(token_to_index.encode_code, max_length)\n",
    "        ),\n",
    "        ref_stream = (e['function_name_tokenized'] for e in data),\n",
    "        ref_encoder = to_torch_encoder(\n",
    "            truncated_encoder(token_to_index.encode_code, max_length)\n",
    "        ),\n",
    "        src_pad_index = pad_index\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_method_name_dataset(train, token_to_index, token_to_index.pad_index, config.max_position_embeddings)\n",
    "valid_dataset = get_method_name_dataset(valid, token_to_index, token_to_index.pad_index, config.max_position_embeddings)\n",
    "beam_dataset = get_method_name_dataset(\n",
    "    random.sample(valid, config.eval_set_size), token_to_index, token_to_index.pad_index, config.max_position_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "def init_scheduler(optimizer, num_epochs, num_steps_epoch, warmup_prop):\n",
    "    num_training_steps = num_steps_epoch * num_epochs + 1\n",
    "    warmup_steps = int(num_training_steps * warmup_prop)\n",
    "    return get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def beam_search(src, model, bos_id, pad_id, end_id, device, max_len=10, k=5):\n",
    "    src = src.view(1,-1).to(device)\n",
    "    src_mask = (src != pad_id).to(device)\n",
    "    \n",
    "    memory = None\n",
    "    \n",
    "    input_seq = [bos_id]\n",
    "    beam = [(input_seq, 0)] \n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "        candidates_proba = []\n",
    "        for snt, snt_proba in beam:\n",
    "            if snt[-1] == end_id:\n",
    "                candidates.append(snt)\n",
    "                candidates_proba.append(snt_proba)\n",
    "            else:    \n",
    "                snt_tensor = torch.tensor(snt).view(1, -1).long().to(device)\n",
    "                \n",
    "                if memory is None:\n",
    "                    memory = model(\n",
    "                        input_ids=src, \n",
    "                        attention_mask=src_mask,\n",
    "                        decoder_input_ids=snt_tensor,\n",
    "                        return_dict=False\n",
    "                    )\n",
    "                else:\n",
    "                    memory = model(\n",
    "                        input_ids=src, \n",
    "                        attention_mask=src_mask,\n",
    "                        decoder_input_ids=snt_tensor,\n",
    "                        encoder_outputs=(memory[1], memory[-1]),\n",
    "                        return_dict=False\n",
    "                    )\n",
    "                    \n",
    "                proba = memory[0].cpu()[0,-1, :]\n",
    "                proba = torch.log_softmax(proba, dim=-1).numpy()\n",
    "                best_k = np.argpartition(-proba, k - 1)[:k]\n",
    "\n",
    "                for tok in best_k:\n",
    "                    candidates.append(snt + [tok])\n",
    "                    candidates_proba.append(snt_proba + proba[tok]) \n",
    "                    \n",
    "        best_candidates = np.argpartition(-np.array(candidates_proba), k - 1)[:k]\n",
    "        beam = [(candidates[j], candidates_proba[j]) for j in best_candidates]\n",
    "        beam = sorted(beam, key=lambda x: -x[1])\n",
    "        \n",
    "    return beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import dl\n",
    "\n",
    "\n",
    "class MethodNameRunner(dl.Runner):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bos_index: int,\n",
    "        pad_index: int,\n",
    "        eos_index: int,\n",
    "        beam_loader_name: str,\n",
    "        num_return_sequences: int,\n",
    "        model=None,\n",
    "        device=None\n",
    "    ):\n",
    "        self.bos_index = bos_index\n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        self.beam_loader_name = beam_loader_name\n",
    "        self.num_return_sequences = num_return_sequences\n",
    "        super().__init__(model=model, device=device)\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        logits = self.model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            decoder_input_ids=batch['decoder_input_ids'],\n",
    "            decoder_attention_mask=batch['decoder_attention_mask']\n",
    "        ).logits\n",
    "        \n",
    "        pad_index = self.model.decoder.config.pad_token_id\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            input=logits.permute(0, 2, 1),\n",
    "            target=batch['labels'],\n",
    "            ignore_index=pad_index\n",
    "        )\n",
    "        self.batch_metrics['loss'] = loss\n",
    "        \n",
    "        if self.loader_name == self.beam_loader_name:\n",
    "            with torch.no_grad():\n",
    "                generated = []\n",
    "                for src in batch['input_ids']:\n",
    "                    gen = beam_search(\n",
    "                        src, \n",
    "                        self.model,\n",
    "                        bos_id=self.bos_index,\n",
    "                        pad_id=self.pad_index,\n",
    "                        end_id=self.eos_index,\n",
    "                        device=self.device,\n",
    "                        max_len=10,\n",
    "                        k=self.num_return_sequences\n",
    "                    )\n",
    "                    generated.append(gen)\n",
    "                self.output = {'generated': generated}\n",
    "\n",
    "\n",
    "class GenerationAccuracyCallback(dl.Callback):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder,\n",
    "        beam_loader_name: str,\n",
    "        num_return_sequences: int,\n",
    "        generated_key: str = 'generated',\n",
    "        target_key: str = 'labels',\n",
    "        prob_print: float = 0.025\n",
    "    ):\n",
    "        super().__init__(order=dl.CallbackOrder.Metric)\n",
    "        self.decoder = decoder\n",
    "        self.beam_loader_name = beam_loader_name\n",
    "        self.num_return_sequences = num_return_sequences\n",
    "        self.generated_key = generated_key\n",
    "        self.target_key = target_key\n",
    "        self.prob_print = prob_print\n",
    "\n",
    "    def on_batch_end(self, runner):\n",
    "        if not runner.loader_name == self.beam_loader_name:\n",
    "            return\n",
    "\n",
    "        labels = runner.input[self.target_key]\n",
    "        generated = runner.output[self.generated_key]\n",
    "        \n",
    "        num_top_n_correct = 0\n",
    "        for lab, candidates in zip(labels, generated):\n",
    "            example = self.decoder.decode(lab)\n",
    "\n",
    "            print_this_example = random.random() < self.prob_print\n",
    "            for cand, prob in candidates:\n",
    "\n",
    "                decoded_cand = self.decoder.decode(cand)\n",
    "                if print_this_example:\n",
    "                    print(\"TARGET:\", example)\n",
    "                    print(\"GENERATED:\", decoded_cand)\n",
    "    \n",
    "                if decoded_cand == example:\n",
    "                    num_top_n_correct += 1\n",
    "                    break\n",
    "\n",
    "        top_n_acc = num_top_n_correct / len(labels)\n",
    "        runner.batch_metrics['accuracy'] = top_n_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': train_dataset.make_loader(batch_size=config.batch_size, shuffle=True),\n",
    "    'valid': valid_dataset.make_loader(batch_size=config.batch_size),\n",
    "    'beam_loader': beam_dataset.make_loader(batch_size=1)\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=config.weight_decay)\n",
    "scheduler = init_scheduler(optimizer, config.num_epochs, len(loaders['train']), config.warmup_prop)\n",
    "callbacks = [\n",
    "    dl.OptimizerCallback(metric_key='loss', accumulation_steps=config.accumulation_steps),\n",
    "    dl.SchedulerCallback(mode='batch'),\n",
    "    dl.EarlyStoppingCallback(patience=config.patience),\n",
    "    dl.WandbLogger(\n",
    "        entity='dimaorekhov',\n",
    "        project='bert4source',\n",
    "        group='method-name-prediction',\n",
    "        name=EXPERIMENT_NAME,\n",
    "        config=vars(config)\n",
    "    ),\n",
    "    dl.CheckpointCallback(resume=config.resume),\n",
    "    GenerationAccuracyCallback(\n",
    "        decoder=token_to_index,\n",
    "        beam_loader_name='beam_loader',\n",
    "        num_return_sequences=config.num_return_sequences\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.logdir).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214754116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embeddings.word_embeddings.weight True\n",
      "encoder.embeddings.position_embeddings.weight True\n",
      "encoder.embeddings.token_type_embeddings.weight True\n",
      "encoder.embeddings.LayerNorm.weight True\n",
      "encoder.embeddings.LayerNorm.bias True\n",
      "encoder.encoder.layer.0.attention.self.query.weight True\n",
      "encoder.encoder.layer.0.attention.self.query.bias True\n",
      "encoder.encoder.layer.0.attention.self.key.weight True\n",
      "encoder.encoder.layer.0.attention.self.key.bias True\n",
      "encoder.encoder.layer.0.attention.self.value.weight True\n",
      "encoder.encoder.layer.0.attention.self.value.bias True\n",
      "encoder.encoder.layer.0.attention.output.dense.weight True\n",
      "encoder.encoder.layer.0.attention.output.dense.bias True\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.0.intermediate.dense.weight True\n",
      "encoder.encoder.layer.0.intermediate.dense.bias True\n",
      "encoder.encoder.layer.0.output.dense.weight True\n",
      "encoder.encoder.layer.0.output.dense.bias True\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.1.attention.self.query.weight True\n",
      "encoder.encoder.layer.1.attention.self.query.bias True\n",
      "encoder.encoder.layer.1.attention.self.key.weight True\n",
      "encoder.encoder.layer.1.attention.self.key.bias True\n",
      "encoder.encoder.layer.1.attention.self.value.weight True\n",
      "encoder.encoder.layer.1.attention.self.value.bias True\n",
      "encoder.encoder.layer.1.attention.output.dense.weight True\n",
      "encoder.encoder.layer.1.attention.output.dense.bias True\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.1.intermediate.dense.weight True\n",
      "encoder.encoder.layer.1.intermediate.dense.bias True\n",
      "encoder.encoder.layer.1.output.dense.weight True\n",
      "encoder.encoder.layer.1.output.dense.bias True\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.2.attention.self.query.weight True\n",
      "encoder.encoder.layer.2.attention.self.query.bias True\n",
      "encoder.encoder.layer.2.attention.self.key.weight True\n",
      "encoder.encoder.layer.2.attention.self.key.bias True\n",
      "encoder.encoder.layer.2.attention.self.value.weight True\n",
      "encoder.encoder.layer.2.attention.self.value.bias True\n",
      "encoder.encoder.layer.2.attention.output.dense.weight True\n",
      "encoder.encoder.layer.2.attention.output.dense.bias True\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.2.intermediate.dense.weight True\n",
      "encoder.encoder.layer.2.intermediate.dense.bias True\n",
      "encoder.encoder.layer.2.output.dense.weight True\n",
      "encoder.encoder.layer.2.output.dense.bias True\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.3.attention.self.query.weight True\n",
      "encoder.encoder.layer.3.attention.self.query.bias True\n",
      "encoder.encoder.layer.3.attention.self.key.weight True\n",
      "encoder.encoder.layer.3.attention.self.key.bias True\n",
      "encoder.encoder.layer.3.attention.self.value.weight True\n",
      "encoder.encoder.layer.3.attention.self.value.bias True\n",
      "encoder.encoder.layer.3.attention.output.dense.weight True\n",
      "encoder.encoder.layer.3.attention.output.dense.bias True\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.3.intermediate.dense.weight True\n",
      "encoder.encoder.layer.3.intermediate.dense.bias True\n",
      "encoder.encoder.layer.3.output.dense.weight True\n",
      "encoder.encoder.layer.3.output.dense.bias True\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.4.attention.self.query.weight True\n",
      "encoder.encoder.layer.4.attention.self.query.bias True\n",
      "encoder.encoder.layer.4.attention.self.key.weight True\n",
      "encoder.encoder.layer.4.attention.self.key.bias True\n",
      "encoder.encoder.layer.4.attention.self.value.weight True\n",
      "encoder.encoder.layer.4.attention.self.value.bias True\n",
      "encoder.encoder.layer.4.attention.output.dense.weight True\n",
      "encoder.encoder.layer.4.attention.output.dense.bias True\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.4.intermediate.dense.weight True\n",
      "encoder.encoder.layer.4.intermediate.dense.bias True\n",
      "encoder.encoder.layer.4.output.dense.weight True\n",
      "encoder.encoder.layer.4.output.dense.bias True\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.5.attention.self.query.weight True\n",
      "encoder.encoder.layer.5.attention.self.query.bias True\n",
      "encoder.encoder.layer.5.attention.self.key.weight True\n",
      "encoder.encoder.layer.5.attention.self.key.bias True\n",
      "encoder.encoder.layer.5.attention.self.value.weight True\n",
      "encoder.encoder.layer.5.attention.self.value.bias True\n",
      "encoder.encoder.layer.5.attention.output.dense.weight True\n",
      "encoder.encoder.layer.5.attention.output.dense.bias True\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.5.intermediate.dense.weight True\n",
      "encoder.encoder.layer.5.intermediate.dense.bias True\n",
      "encoder.encoder.layer.5.output.dense.weight True\n",
      "encoder.encoder.layer.5.output.dense.bias True\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias True\n",
      "encoder.pooler.dense.weight True\n",
      "encoder.pooler.dense.bias True\n",
      "decoder.bert.embeddings.word_embeddings.weight True\n",
      "decoder.bert.embeddings.position_embeddings.weight True\n",
      "decoder.bert.embeddings.token_type_embeddings.weight True\n",
      "decoder.bert.embeddings.LayerNorm.weight True\n",
      "decoder.bert.embeddings.LayerNorm.bias True\n",
      "decoder.bert.encoder.layer.0.attention.self.query.weight True\n",
      "decoder.bert.encoder.layer.0.attention.self.query.bias True\n",
      "decoder.bert.encoder.layer.0.attention.self.key.weight True\n",
      "decoder.bert.encoder.layer.0.attention.self.key.bias True\n",
      "decoder.bert.encoder.layer.0.attention.self.value.weight True\n",
      "decoder.bert.encoder.layer.0.attention.self.value.bias True\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.weight True\n",
      "decoder.bert.encoder.layer.0.attention.output.dense.bias True\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.weight True\n",
      "decoder.bert.encoder.layer.0.crossattention.self.query.bias True\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.weight True\n",
      "decoder.bert.encoder.layer.0.crossattention.self.key.bias True\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.weight True\n",
      "decoder.bert.encoder.layer.0.crossattention.self.value.bias True\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.weight True\n",
      "decoder.bert.encoder.layer.0.crossattention.output.dense.bias True\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight True\n",
      "decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias True\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.weight True\n",
      "decoder.bert.encoder.layer.0.intermediate.dense.bias True\n",
      "decoder.bert.encoder.layer.0.output.dense.weight True\n",
      "decoder.bert.encoder.layer.0.output.dense.bias True\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.weight True\n",
      "decoder.bert.encoder.layer.0.output.LayerNorm.bias True\n",
      "decoder.bert.encoder.layer.1.attention.self.query.weight True\n",
      "decoder.bert.encoder.layer.1.attention.self.query.bias True\n",
      "decoder.bert.encoder.layer.1.attention.self.key.weight True\n",
      "decoder.bert.encoder.layer.1.attention.self.key.bias True\n",
      "decoder.bert.encoder.layer.1.attention.self.value.weight True\n",
      "decoder.bert.encoder.layer.1.attention.self.value.bias True\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.weight True\n",
      "decoder.bert.encoder.layer.1.attention.output.dense.bias True\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.weight True\n",
      "decoder.bert.encoder.layer.1.crossattention.self.query.bias True\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.weight True\n",
      "decoder.bert.encoder.layer.1.crossattention.self.key.bias True\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.weight True\n",
      "decoder.bert.encoder.layer.1.crossattention.self.value.bias True\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.weight True\n",
      "decoder.bert.encoder.layer.1.crossattention.output.dense.bias True\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight True\n",
      "decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias True\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.weight True\n",
      "decoder.bert.encoder.layer.1.intermediate.dense.bias True\n",
      "decoder.bert.encoder.layer.1.output.dense.weight True\n",
      "decoder.bert.encoder.layer.1.output.dense.bias True\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.weight True\n",
      "decoder.bert.encoder.layer.1.output.LayerNorm.bias True\n",
      "decoder.cls.predictions.bias True\n",
      "decoder.cls.predictions.transform.dense.weight True\n",
      "decoder.cls.predictions.transform.dense.bias True\n",
      "decoder.cls.predictions.transform.LayerNorm.weight True\n",
      "decoder.cls.predictions.transform.LayerNorm.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimaorekhov\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">seq2seq-transformer</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/dimaorekhov/bert4source\" target=\"_blank\">https://wandb.ai/dimaorekhov/bert4source</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/dimaorekhov/bert4source/runs/1ck7d346\" target=\"_blank\">https://wandb.ai/dimaorekhov/bert4source/runs/1ck7d346</a><br/>\n",
       "                Run data is saved locally in <code>logdir_seq2seq-transformer/wandb/run-20201222_025342-1ck7d346</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50 * Epoch (train):   0% 1/12733 [00:00<3:05:35,  1.14it/s, loss=10.933, lr=1.047e-08, momentum=0.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxkvant/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50 * Epoch (train):   1% 106/12733 [01:25<2:53:56,  1.21it/s, loss=10.747, lr=1.110e-06, momentum=0.900]"
     ]
    }
   ],
   "source": [
    "runner = MethodNameRunner(\n",
    "    bos_index=token_to_index.bos_index,\n",
    "    pad_index=token_to_index.pad_index,\n",
    "    eos_index=token_to_index.pad_index,\n",
    "    beam_loader_name='beam_loader',\n",
    "    num_return_sequences=config.num_return_sequences,\n",
    "    device=torch.device(\"cuda\")\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=config.num_epochs,\n",
    "    logdir=config.logdir,\n",
    "    verbose=True,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
